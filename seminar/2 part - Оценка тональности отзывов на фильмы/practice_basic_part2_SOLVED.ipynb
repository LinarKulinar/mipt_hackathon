{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Зимняя школа МФТИ\n",
    "# Практикум по машинному обучению: базовый поток\n",
    "*Автор: [Илья Захаркин](https://vk.com/ilyazakharkin) | @izakharkin*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка тональности отзывов на фильмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh4.googleusercontent.com/proxy/1sG9fGlYWYqxND3cM-lrddATFg1cQHUI3GjtP19mtfn8qiSXcli4PbDxdFUk9z1EfxomhFkng8UNlgePX5tpLKVjC8jO7nF-K1BQJHoXjP9ivlv9AX3hsqvC4BRRmSYK4uP2zvQ5Ap1OH5k\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3260/1*8XIjunF2z6dmsVlkEuOUaw.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом ноутбуке мы проанализируем датасет отзывов с *IMDB*, будем оценивать их позитивность (1) и негативность (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: работа с категориальными признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.DataFrame(data={'Фрукт': ['яблоко', 'груша', 'яблоко', 'банан', 'киви']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Фрукт</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>груша</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>банан</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>киви</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Фрукт\n",
       "0  яблоко\n",
       "1   груша\n",
       "2  яблоко\n",
       "3   банан\n",
       "4    киви"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "result = encoder.fit_transform(fruits.values.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Фрукт</th>\n",
       "      <th>LabelEncoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>яблоко</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>груша</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>яблоко</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>банан</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>киви</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Фрукт  LabelEncoder\n",
       "0  яблоко             3\n",
       "1   груша             1\n",
       "2  яблоко             3\n",
       "3   банан             0\n",
       "4    киви             2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits['LabelEncoder'] = result\n",
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OneHotEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "result = encoder.fit_transform(fruits['Фрукт'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Фрукт</th>\n",
       "      <th>LabelEncoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>яблоко</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>груша</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>яблоко</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>банан</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>киви</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Фрукт  LabelEncoder\n",
       "0  яблоко             3\n",
       "1   груша             1\n",
       "2  яблоко             3\n",
       "3   банан             0\n",
       "4    киви             2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скачиваем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/izakharkin/opt/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/izakharkin/opt/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/izakharkin/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Информация о датасете::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n",
      "=======\n",
      "\n",
      "Introduction\n",
      "\n",
      "This README v2.0 (June, 2004) for the v2.0 polarity dataset comes from\n",
      "the URL http://www.cs.cornell.edu/people/pabo/movie-review-data .\n",
      "\n",
      "=======\n",
      "\n",
      "What's New -- June, 2004\n",
      "\n",
      "This dataset represents an enhancement of the review corpus v1.0\n",
      "described in README v1.1: it contains more reviews, and labels were\n",
      "created with an improved rating-extraction system.\n",
      "\n",
      "=======\n",
      "\n",
      "Citation Info \n",
      "\n",
      "This data was first used in Bo Pang and Lillian Lee,\n",
      "``A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization \n",
      "Based on Minimum Cuts'',  Proceedings of the ACL, 2004.\n",
      "\n",
      "@InProceedings{Pang+Lee:04a,\n",
      "  author =       {Bo Pang and Lillian Lee},\n",
      "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
      "  booktitle =    \"Proceedings of the ACL\",\n",
      "  year =         2004\n",
      "}\n",
      "\n",
      "=======\n",
      "\n",
      "Data Format Summary \n",
      "\n",
      "- review_polarity.tar.gz: contains this readme and  data used in\n",
      "  the experiments described in Pang/Lee ACL 2004.\n",
      "\n",
      "  Specifically:\n",
      "\n",
      "  Within the folder \"txt_sentoken\" are the 2000 processed down-cased\n",
      "  text files used in Pang/Lee ACL 2004; the names of the two\n",
      "  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\n",
      "  classification (sentiment) of the component files according to our\n",
      "  automatic rating classifier (see section \"Rating Decision\" below).\n",
      "\n",
      "  File names consist of a cross-validation tag plus the name of the\n",
      "  original html file.  The ten folds used in the Pang/Lee ACL 2004 paper's\n",
      "  experiments were:\n",
      "\n",
      "     fold 1: files tagged cv000 through cv099, in numerical order\n",
      "     fold 2: files tagged cv100 through cv199, in numerical order     \n",
      "     ...\n",
      "     fold 10: files tagged cv900 through cv999, in numerical order\n",
      "\n",
      "  Hence, the file neg/cv114_19501.txt, for example, was labeled as\n",
      "  negative, served as a member of fold 2, and was extracted from the\n",
      "  file 19501.html in polarity_html.zip (see below).\n",
      "\n",
      "  Each line in each text file corresponds to a single sentence, as\n",
      "  determined by Adwait Ratnaparkhi's sentence boundary detector\n",
      "  MXTERMINATOR.\n",
      " \n",
      "  Preliminary steps were taken to remove rating information from the\n",
      "  text files, but only the rating information upon which the rating\n",
      "  decision was based is guaranteed to have been removed. Thus, if the\n",
      "  original review contains several instances of rating information,\n",
      "  potentially given in different forms, those not recognized as valid\n",
      "  ratings remain part of the review text.\n",
      "\t\n",
      "- polarity_html.zip: The original source files from which the\n",
      "  processed, labeled, and (randomly) selected data in\n",
      "  review_polarity.tar.gz was derived.\n",
      "\n",
      "  Specifically:  \n",
      "\n",
      "  This data consists of unprocessed, unlabeled html files from the\n",
      "  IMDb archive of the rec.arts.movies.reviews newsgroup,\n",
      "  http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz\n",
      "  represent a processed subset of these files. \n",
      "\n",
      "=======\n",
      "\n",
      "Rating Decision (Appendix A)\n",
      "\n",
      "This section describes how we determined whether a review was positive\n",
      "or negative.\n",
      "\n",
      "The original html files do not have consistent formats -- a review may\n",
      "not have the author's rating with it, and when it does, the rating can\n",
      "appear at different places in the file in different forms.  We only\n",
      "recognize some of the more explicit ratings, which are extracted via a\n",
      "set of ad-hoc rules.  In essence, a file's classification is determined\n",
      "based on the first rating we were able to identify.\n",
      "\n",
      "\n",
      "- In order to obtain more accurate rating decisions, the maximum\n",
      "\trating must be specified explicitly, both for numerical ratings\n",
      "\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\n",
      "\t****: ***\" are examples of rating indications we recognize.)\n",
      "\n",
      "- With a five-star system (or compatible number systems):\n",
      "\tthree-and-a-half stars and up are considered positive, \n",
      "\ttwo stars and below are considered negative.\n",
      "- With a four-star system (or compatible number system):\n",
      "\tthree stars and up are considered positive, \n",
      "\tone-and-a-half stars and below are considered negative.  \n",
      "- With a letter grade system:\n",
      "\tB or above is considered positive,\n",
      "\tC- or below is considered negative.\n",
      "\n",
      "We attempted to recognize half stars, but they are specified in an\n",
      "especially free way, which makes them difficult to recognize.  Hence,\n",
      "we may lose a half star very occasionally; but this only results in 2.5\n",
      "stars in five star system being categorized as negative, which is \n",
      "still reasonable.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot : two teen couples go to a church party , drink and then drive . ',\n",
       " 'they get into an accident . ',\n",
       " 'one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . ',\n",
       " \"what's the deal ? \",\n",
       " 'watch the movie and \" sorta \" find out . . . ',\n",
       " 'critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . ',\n",
       " \"which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \",\n",
       " 'they seem to have taken this pretty neat concept , but executed it terribly . ',\n",
       " 'so what are the problems with the movie ? ',\n",
       " \"well , its main problem is that it's simply too jumbled . \"]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.raw().split('\\n')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Чтобы удобно было разделять отзывы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "negfeats = [movie_reviews.words(fileids=[f]) for f in negids]\n",
    "posfeats = [movie_reviews.words(fileids=[f]) for f in posids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...],\n",
       " ['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...],\n",
       " ['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...],\n",
       " ['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...],\n",
       " ['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negfeats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', '!', 'what', 'a', 'movie', '.', 'it', \"'\", 's', ...],\n",
       " ['richard', 'gere', 'can', 'be', 'a', 'commanding', ...],\n",
       " ['glory', '--', 'starring', 'matthew', 'broderick', ...],\n",
       " ['steven', 'spielberg', \"'\", 's', 'second', 'epic', ...],\n",
       " ['truman', '(', '\"', 'true', '-', 'man', '\"', ')', ...]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posfeats[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Саздаём **DataFrame** и вектор меток **y**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = pd.DataFrame({'sentiment': negfeats + posfeats})  # all sentiments\n",
    "labels = np.array([0] * len(negfeats) + [1] * len(posfeats))  # 0 - bad, 1 - good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(plot, :, two, teen, couples, go, to, a, churc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(the, happy, bastard, ', s, quick, movie, revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(it, is, movies, like, these, that, make, a, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(\", quest, for, camelot, \", is, warner, bros, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(synopsis, :, a, mentally, unstable, man, unde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentiment\n",
       "0  (plot, :, two, teen, couples, go, to, a, churc...\n",
       "1  (the, happy, bastard, ', s, quick, movie, revi...\n",
       "2  (it, is, movies, like, these, that, make, a, j...\n",
       "3  (\", quest, for, camelot, \", is, warner, bros, ...\n",
       "4  (synopsis, :, a, mentally, unstable, man, unde..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1995</td>\n",
       "      <td>(wow, !, what, a, movie, ., it, ', s, everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>(richard, gere, can, be, a, commanding, actor,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1997</td>\n",
       "      <td>(glory, --, starring, matthew, broderick, ,, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>(steven, spielberg, ', s, second, epic, film, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1999</td>\n",
       "      <td>(truman, (, \", true, -, man, \", ), burbank, is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentiment\n",
       "1995  (wow, !, what, a, movie, ., it, ', s, everythi...\n",
       "1996  (richard, gere, can, be, a, commanding, actor,...\n",
       "1997  (glory, --, starring, matthew, broderick, ,, d...\n",
       "1998  (steven, spielberg, ', s, second, epic, film, ...\n",
       "1999  (truman, (, \", true, -, man, \", ), burbank, is..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posfeats) / (len(negfeats) + len(posfeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование данных в ***Bag-of-Words*** (\"мешок слов\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2552/1*MeSYCKGDOdwkJKVZKxJuvg.png\" width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [' '.join(sent) for sent in sentiments['sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cnt_vect.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x39659 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 666842 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробуем логистическую регрессию как модель для классификации тональности отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_logreg = Pipeline([('vectorizer', cnt_vect), ('logreg', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Измерим качество **accuracy score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = cross_val_score(vect_logreg, data, y, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360216503929078"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Измерим качество с помощью **ROC AUC**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_aucs = cross_val_score(vect_logreg, data, y, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107764937833774"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(roc_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучаем модель на *всех данных*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-5 наиболее важных слов для классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2954, 37056, 39195, 14159, 38417])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = np.argsort(np.abs(clf.coef_))[0][::-1][:5]\n",
    "idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad', 'unfortunately', 'worst', 'fun', 'waste'], dtype='<U58')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cnt_vect.get_feature_names())[idx_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспериментируем с моделями, stop-words и n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Давайте проверим, какое качество даст бейзлайн **без stop-words и n-grams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_logreg_pipe = Pipeline([('vectorizer', CountVectorizer()), ('logreg', LogisticRegression())])\n",
    "tfidf_logreg_pipe = Pipeline([('vectorizer', TfidfVectorizer()), ('logreg', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_scores_count = cross_val_score(count_logreg_pipe, data, y, cv=5)\n",
    "cv_acc_scores_tfidf = cross_val_score(tfidf_logreg_pipe, data, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression CV accuracy:     0.84100 (0.01678)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'CountVectorizer + LogisticRegression CV accuracy: \\\n",
    "    {cv_acc_scores_count.mean():.5f} ({cv_acc_scores_count.std():.5f})'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdfVectorizer + LogisticRegression CV accuracy:     0.82100 (0.00406)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'TfIdfVectorizer + LogisticRegression CV accuracy: \\\n",
    "    {cv_acc_scores_tfidf.mean():.5f} ({cv_acc_scores_tfidf.std():.5f})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Теперь изменим *min_df* у *CountVectorizer* и увидим, как изменится модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression CV accuracy with min_df=10:         0.839 (0.012)\n",
      "\n",
      "CountVectorizer + LogisticRegression CV accuracy with min_df=20:         0.833 (0.017)\n",
      "\n",
      "CountVectorizer + LogisticRegression CV accuracy with min_df=30:         0.825 (0.014)\n",
      "\n",
      "CountVectorizer + LogisticRegression CV accuracy with min_df=40:         0.817 (0.009)\n",
      "\n",
      "CountVectorizer + LogisticRegression CV accuracy with min_df=50:         0.813 (0.013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = [10, 20, 30, 40, 50]\n",
    "\n",
    "for min_df in param_grid:\n",
    "    count_logreg_pipe = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(min_df=min_df)), \n",
    "        ('logreg', LogisticRegression())\n",
    "    ])\n",
    "    cv_acc_scores_count = cross_val_score(count_logreg_pipe, data, y, cv=5)\n",
    "    print(\n",
    "        f'CountVectorizer + LogisticRegression CV accuracy with min_df={min_df}: \\\n",
    "        {cv_acc_scores_count.mean():.3f} ({cv_acc_scores_count.std():.3f})\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Попробуем разные модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(solver='lbfgs'), LinearSVC(), SGDClassifier()]\n",
    "pipelines = [Pipeline([('vectorizer', CountVectorizer()), ('classifier', clf)]) for clf in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.842 (0.022)\n",
      "\n",
      "CV accuracy: 0.833 (0.016)\n",
      "\n",
      "CV accuracy: 0.834 (0.013)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipeline in pipelines:\n",
    "    cv_acc_scores = cross_val_score(pipeline, data, y, cv=5)\n",
    "    print(f'CV accuracy: {cv_acc_scores.mean():.3f} ({cv_acc_scores.std():.3f})\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Сравним **stop-words** из **sklearn** и из **nltk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/izakharkin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_nltk = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression + stop-words: sklearn CV accuracy:         0.839 (0.010)\n",
      "\n",
      "CountVectorizer + LogisticRegression + stop-words: nltk CV accuracy:         0.841 (0.010)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['english', stop_words_nltk]\n",
    "\n",
    "pipelines = [\n",
    "    Pipeline([\n",
    "        ('vectorizer', CountVectorizer(stop_words=words)), \n",
    "        ('logreg', LogisticRegression())\n",
    "    ]) \n",
    "    for words in stop_words\n",
    "]\n",
    "\n",
    "names = ['sklearn', 'nltk']\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    cv_acc_scores = cross_val_score(pipeline, data, y, cv=5)\n",
    "    print(\n",
    "        f'CountVectorizer + LogisticRegression + stop-words: {names[i]} CV accuracy: \\\n",
    "        {cv_acc_scores.mean():.3f} ({cv_acc_scores.std():.3f})\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* И, наконец, попробуем **1-2-word-grams** и **3-5-char-grams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression + 1-2-word-grams CV accuracy:     0.853 (0.017)\n"
     ]
    }
   ],
   "source": [
    "cv_acc_scores = cross_val_score(pipeline, data, y, cv=5)\n",
    "print(\n",
    "    f'CountVectorizer + LogisticRegression + 1-2-word-grams CV accuracy: \\\n",
    "    {cv_acc_scores.mean():.3f} ({cv_acc_scores.std():.3f})'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer='char_wb', ngram_range=(3, 5))), \n",
    "    ('logreg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + LogisticRegression + 3-5-char-grams CV accuracy:     0.820 (0.011)\n"
     ]
    }
   ],
   "source": [
    "cv_acc_scores = cross_val_score(pipeline, data, y, cv=5)\n",
    "print(\n",
    "    f'CountVectorizer + LogisticRegression + 3-5-char-grams CV accuracy: \\\n",
    "    {cv_acc_scores.mean():.3f} ({cv_acc_scores.std():.3f})'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning School при ФПМИ МФТИ:\n",
    "* [Официальный сайт](https://www.dlschool.org/) | [Github](https://github.com/DLSchool/deep_learning_2018-19) | [YouTube](https://www.youtube.com/channel/UCFTNoZYjkg-3LZTHrHfV1nQ/) | [VK](https://vk.com/dlschool_mipt) | [Telegram]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Курсы:\n",
    "* [Специализация Яндекса по Анализу данных и Машинному обучению](https://ru.coursera.org/specializations/machine-learning-data-analysis)\n",
    "* [Открытый курс по машинному обучению от OpenDataScience](https://habr.com/ru/company/ods/blog/322626/)\n",
    "* [deeplearning.ai](https://www.coursera.org/specializations/deep-learning) по Deep Learning (нейронные сети)\n",
    "* [Stanford cs231n](http://cs231n.stanford.edu/) по Computer Vision\n",
    "* [Stanford cs224n](http://web.stanford.edu/class/cs224n/) по Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Демо:\n",
    "* [Сайт-сборник интерактивных демо по машинному обучению](http://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
